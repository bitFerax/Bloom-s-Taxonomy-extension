{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/ferax/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/ferax/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/ferax/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/ferax/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BT 2 - Data Collection.ipynb\n",
    "Author: Jingchuan Shi\n",
    "Acknowledgments: Asst. Prof. Ahmed Qureshi\n",
    "Created 2019/9/7, last modified 2019/9/9 at University of Alberta.\n",
    "All Rights Reserved.\n",
    "'''\n",
    "\n",
    "# Load relevant modules.\n",
    "import os\n",
    "import stanfordnlp\n",
    "from tika import parser\n",
    "from nltk.corpus import wordnet as wn\n",
    "config = {\n",
    "        'processors': 'tokenize,pos,lemma'\n",
    "         }\n",
    "nlp = stanfordnlp.Pipeline(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current data size: 6124\n",
      "Verbs identified by wordnet: 3079\n"
     ]
    }
   ],
   "source": [
    "# Files that store verbs and information about the data collection process. Please modify the master path to your own.\n",
    "master_path = '/Users/ferax/bin/'\n",
    "core_source_path = master_path + 'BTverblist_core.txt' # List of pre-labelled words.\n",
    "new_source_path = master_path + 'BTverblist_new.txt' # List of new words to be extracted and analyzed.\n",
    "directory = master_path + 'Paper_source/' # Directory of all papers and books that are used as corpus.\n",
    "done_source_path = master_path + 'BTdone_paper_list.txt' # List of papers and books that are already extracted.\n",
    "progress_path = master_path + 'BTgain_progress.txt' # Tracks the progress of data extraction.\n",
    "\n",
    "# Read from files\n",
    "with open(core_source_path, 'r') as f:\n",
    "    core_verbs = [line.replace('\\n', '') for line in f.readlines()]\n",
    "with open(new_source_path, 'r') as g:\n",
    "    verbs = [line.replace('\\n', '') for line in g.readlines()]\n",
    "    old_size = len(verbs)\n",
    "    print('\\nCurrent data size: ', end = '') # Amount of words before the current extraction session.\n",
    "    print(old_size)\n",
    "with open(done_source_path, 'r') as h:\n",
    "    done_papers = [line.replace('\\n', '') for line in h.readlines()]\n",
    "files = [os.path.join(directory, file) for file in os.listdir(directory) if os.path.join(directory, file) not in done_papers]\n",
    "\n",
    "# The main extraction procedure\n",
    "for file_path in files:\n",
    "    if file_path[-3:] == 'pdf': # PDFs are parsed with tika.\n",
    "        raw = parser.from_file(file_path)\n",
    "        raw_text = raw['content']\n",
    "        if raw_text == None:\n",
    "            with open(done_source_path, 'a') as n:\n",
    "                n.write(file_path)\n",
    "                n.write('\\n')\n",
    "            continue\n",
    "        else:\n",
    "            raw_text = raw_text.replace('-\\n', '').replace('\\n', ' ')\n",
    "    elif file_path[-3:] == 'txt': # Plain texts are read directly.\n",
    "        with open(file_path, 'r') as l:\n",
    "            raw_text = l.read()\n",
    "            if raw_text == None:\n",
    "                with open(done_source_path, 'a') as n:\n",
    "                    n.write(file_path)\n",
    "                    n.write('\\n')\n",
    "                continue\n",
    "            else:\n",
    "                raw_text = raw_text.replace('-\\n', '').replace('\\n', ' ')\n",
    "    else:\n",
    "        continue\n",
    "    new_verbs = []\n",
    "    stanford_obj = nlp(raw_text) # Raw texts are tokenized, pos-tagged and lemmatized by StanfordNLP.\n",
    "    processed = [[word.lemma, word.xpos] for sent in stanford_obj.sentences for word in sent.words]\n",
    "    for word in processed: # Words recognized as verbs are added to the list. Only unique verbs are allowed.\n",
    "        if word[0] == None:\n",
    "            continue\n",
    "        else:\n",
    "            word[0] = word[0].lower()\n",
    "            if (word[1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] and word[0] not in core_verbs and word[0] not in verbs and word[0] not in new_verbs):\n",
    "                new_verbs.append(word[0])\n",
    "    with open(new_source_path, 'a') as m: # Write the newly extracted words to file.\n",
    "        for word in new_verbs:\n",
    "            m.write(word)\n",
    "            m.write('\\n')\n",
    "            verbs.append(word)\n",
    "    with open(done_source_path, 'a') as n: # The current paper or book is added to the finished list and will not be analyzed again.\n",
    "        n.write(file_path)\n",
    "        n.write('\\n')\n",
    "    print('\\nLoaded paper {}.'.format(file_path)) # Report progress.\n",
    "    print('Current data size: ', end = '')\n",
    "    print(len(verbs), end = '')\n",
    "    gain = len(verbs) - old_size\n",
    "    print('. {} new verbs acquired.'.format(gain))\n",
    "    old_size = len(verbs)\n",
    "    with open(progress_path, 'a') as n: # Write the amount of newly extracted words and amount of all words so far to the progress tracking file.\n",
    "        n.write(str(len(verbs)) + ' ' + str(gain) + '\\n')\n",
    "\n",
    "valid_verb = [] \n",
    "for word in verbs: # Validate extracted words by checking if it has any meaningful WordNet synset as a verb.\n",
    "    if wn.synsets(word, pos = 'v') != []:\n",
    "        valid_verb.append(word)\n",
    "print('# of verbs identified by wordnet: ' + str(len(valid_verb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
